# a name, the connector class to run, and the maximum number of tasks to create:
name=yb-YB-sink_t2
connector.class=io.yellowbrick.kafka.YBSinkConnector
tasks.max=1

# The topics to consume from - required for sink connectors like this one
topics=connect-source

# Configuration
yb.hostname=trialsandbox.sandbox.aws.yellowbrickcloud.com
yb.port=5432
yb.database=odl_user_XXXXXXX_db
yb.username=odl_user_XXXXXXX@yellowbrickcloud.com
yb.password=apbx91DTW*8h
yb.table=kafka_ybd_source_load
yb.schema=kafka
yb.columns=col1,col2

yb.bad.row.columns=col1,YBLOAD_ERROR_COLUMN,YBLOAD_ERROR_REASON
#yb.batchsize=

yb.writer=BINARY
#yb.writer=CSV
#yb.writer=RELAY

yb.relay.hostname=localhost
yb.relay.port=21212
yb.zookeeper=localhost:2181

# Computed column examples.
yb.computed.columns=data_column,composed_column,timestamp_column,operation_column
yb.computed.column.data_column='topic:' + sinkRecord.topic() + '|partition:' + sinkRecord.kafkaPartition() + '|offset:' + sinkRecord.kafkaOffset()
#yb.computed.column.composed_column=sinkRecord.value().get('id') + '|' + sinkRecord.value().get('name')
yb.computed.column.composed_column=sinkRecord.value().get('col1')
yb.computed.column.timestamp_column=sinkRecord.timestamp()
yb.computed.column.operation_column=sinkRecord.headers().?lastWithName('__debezium-operation').?value()

# Post commit example.
#yb.post.commit.sql=update @{table} set partition_column='@{topicMap.toString().replace(\"''\", \"\")}' where partition_column is null

# Debezium transform to specify that its CDC record should be flattened and its operation supplied as header
#transforms=unwrap
#transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState
#transforms.unwrap.drop.tombstones=false
#transforms.unwrap.delete.handling.mode=rewrite
#transforms.unwrap.delete.handling.mode=drop
#transforms.unwrap.operation.header=true

