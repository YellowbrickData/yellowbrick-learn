{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f364c650-99eb-46cc-bc30-2e5a020e9971",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import psycopg2\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"StoreSalesAnalytics\").getOrCreate()\n",
    "\n",
    "\n",
    "# Read the store_sales table from Yellowbrick, limited to 10k rows, sorted by ss_sold_date_sk\n",
    "#Be sure to replace the user and password to the credentials you get from your sandbox sign up.\n",
    "store_sales_pyspark = (spark.read.format(\"postgresql\")\n",
    "  .option(\"dbtable\", \"(SELECT * FROM tpcds_sf1000.store_sales LIMIT 10000) AS subquery\")\n",
    "  .option(\"host\", \"trialsandbox.sandbox.aws.yellowbrickcloud.com\")\n",
    "  .option(\"port\", \"5432\")\n",
    "  .option(\"database\", \"sample_data\")\n",
    "  .option(\"user\", \"your_user\")\n",
    "  .option(\"password\", \"your_password\")\n",
    "  .load())\n",
    "\n",
    "\n",
    "# Perform the analytical transformations\n",
    "window = Window.partitionBy(\"ss_sold_date_sk\", \"ss_customer_sk\").orderBy(desc(\"ss_net_profit\"))\n",
    "store_sales_analytics = store_sales_pyspark.select(\n",
    "    \"ss_sold_date_sk\",\n",
    "    \"ss_sold_time_sk\",\n",
    "    \"ss_item_sk\",\n",
    "    \"ss_customer_sk\",\n",
    "    \"ss_cdemo_sk\",\n",
    "    \"ss_hdemo_sk\",\n",
    "    \"ss_addr_sk\",\n",
    "    \"ss_store_sk\",\n",
    "    \"ss_promo_sk\",\n",
    "    \"ss_ticket_number\",\n",
    "    \"ss_quantity\",\n",
    "    \"ss_wholesale_cost\",\n",
    "    \"ss_list_price\",\n",
    "    \"ss_sales_price\",\n",
    "    \"ss_ext_discount_amt\",\n",
    "    \"ss_ext_sales_price\",\n",
    "    \"ss_ext_wholesale_cost\",\n",
    "    \"ss_ext_list_price\",\n",
    "    \"ss_ext_tax\",\n",
    "    \"ss_coupon_amt\",\n",
    "    \"ss_net_paid\",\n",
    "    \"ss_net_paid_inc_tax\",\n",
    "    \"ss_net_profit\",\n",
    "    row_number().over(window).alias(\"rn\"),\n",
    "    rank().over(window).alias(\"rnk\"),\n",
    "    dense_rank().over(window).alias(\"dense_rnk\"),\n",
    "    lead(\"ss_net_profit\", 1).over(window).alias(\"next_profit\"),\n",
    "    lag(\"ss_net_profit\", 1).over(window).alias(\"prev_profit\"),\n",
    "    round(col(\"ss_sales_price\") / col(\"ss_list_price\"), 2).alias(\"price_discount_pct\"),\n",
    "    round(col(\"ss_ext_sales_price\") / col(\"ss_ext_list_price\"), 2).alias(\"total_discount_pct\"),\n",
    "    round(col(\"ss_net_profit\") / col(\"ss_ext_sales_price\"), 2).alias(\"profit_margin\"),\n",
    "    when(col(\"ss_net_profit\") > 0, lit(1)).otherwise(lit(0)).cast(\"NUMERIC\").alias(\"profitability_status\")\n",
    ")\n",
    "\n",
    "# Define Yellowbrick connection details\n",
    "# Be sure to replace the user and password to the credentials you get from your sandbox sign up\n",
    "driver = \"org.postgresql.Driver\"\n",
    "database_host = \"trialsandbox.sandbox.aws.yellowbrickcloud.com\"\n",
    "database_port = \"5432\"\n",
    "database_name = \"sample_data\"\n",
    "table = \"tpcds_sf1000.store_sales_analytics\"\n",
    "user = \"your_user\"\n",
    "password = \"your_password\"\n",
    "\n",
    "# Construct the Postgres URL for the custom connector\n",
    "url = f\"jdbc:postgresql://{database_host}:{database_port}/{database_name}?user={user}&password={password}\"\n",
    "\n",
    "\n",
    "# Create the store_sales_analytics table in PostgreSQL using psycopg2\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tpcds_sf1000.store_sales_analytics (\n",
    "    ss_sold_date_sk INTEGER,\n",
    "    ss_sold_time_sk INTEGER,\n",
    "    ss_item_sk INTEGER,\n",
    "    ss_customer_sk INTEGER,\n",
    "    ss_cdemo_sk INTEGER,\n",
    "    ss_hdemo_sk INTEGER,\n",
    "    ss_addr_sk INTEGER,\n",
    "    ss_store_sk INTEGER,\n",
    "    ss_promo_sk INTEGER,\n",
    "    ss_ticket_number INTEGER,\n",
    "    ss_quantity INTEGER,\n",
    "    ss_wholesale_cost NUMERIC,\n",
    "    ss_list_price NUMERIC,\n",
    "    ss_sales_price NUMERIC,\n",
    "    ss_ext_discount_amt NUMERIC,\n",
    "    ss_ext_sales_price NUMERIC,\n",
    "    ss_ext_wholesale_cost NUMERIC,\n",
    "    ss_ext_list_price NUMERIC,\n",
    "    ss_ext_tax NUMERIC,\n",
    "    ss_coupon_amt NUMERIC,\n",
    "    ss_net_paid NUMERIC,\n",
    "    ss_net_paid_inc_tax NUMERIC,\n",
    "    ss_net_profit NUMERIC,\n",
    "    rn INTEGER,\n",
    "    rnk INTEGER,\n",
    "    dense_rnk INTEGER,\n",
    "    next_profit NUMERIC,\n",
    "    prev_profit NUMERIC,\n",
    "    price_discount_pct NUMERIC,\n",
    "    total_discount_pct NUMERIC,\n",
    "    profit_margin NUMERIC,\n",
    "    profitability_status NUMERIC\n",
    ") Distribute random;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the create table command using psycopg2\n",
    "conn = psycopg2.connect(host=database_host, port=database_port, dbname=database_name, user=user, password=password)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Write the DataFrame to the table using the Postgres Spark connector\n",
    "(store_sales_analytics.write\n",
    "    .format(\"postgresql\")\n",
    "    .option(\"host\", database_host)\n",
    "    .option(\"port\", database_port)\n",
    "    .option(\"database\", database_name)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"dbtable\", table)\n",
    "    .mode(\"append\")  # Append the data to the existing table\n",
    "    .save()\n",
    ")\n",
    "\n",
    "# Execute the count query\n",
    "cursor.execute(\"SELECT COUNT(*) AS row_count FROM tpcds_sf1000.store_sales_analytics;\")\n",
    "\n",
    "# Fetch the result\n",
    "result = cursor.fetchone()\n",
    "row_count = result[0]\n",
    "\n",
    "# Output the row count\n",
    "print(f\"Row count after insertion: {row_count}\")\n",
    "\n",
    "# Clean up by dropping the store_sales_analytics table\n",
    "cursor.execute(\"DROP TABLE IF EXISTS tpcds_sf1000.store_sales_analytics\")\n",
    "conn.commit()\n",
    "\n",
    "# Close cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Cleanup: 'store_sales_analytics' table has been dropped.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Read from Store Sales and Load into Store Sales Analytics(Spark Processing with DataFrames)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
